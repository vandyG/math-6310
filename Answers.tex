\documentclass{amsart}[11pt]
\usepackage[utf8]{inputenc}

\pagestyle{plain}

\usepackage{hyperref}
%\usepackage{tablefootnote}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{mathrsfs}
\usepackage{float}
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{comment}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{ulem}
\usepackage[dvipsnames]{xcolor}
\usepackage[lined,boxed,commentsnumbered]{algorithm2e}
\usepackage{fourier}
\usepackage{multicol}
\usepackage{soul}
\allowdisplaybreaks

\hypersetup{
	colorlinks,
	citecolor=blue,
	filecolor=blue,
	linkcolor=blue,
	urlcolor=blue,
	%hyperfootnotes=false
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{lem}{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}

\newtheorem*{claim}{Claim}
\newtheorem{thmnum}{Theorem}
\renewcommand{\thethmnum}{\Alph{thmnum}}
\newtheorem{propnum}{Proposition}
\renewcommand{\thepropnum}{\Alph{propnum}}
\newtheorem{cornum}{Corollary}
\renewcommand{\thecornum}{\Alph{cornum}}

\newtheorem*{theoremos}{Theorem}
\newtheorem*{prop}{Proposition}
\theoremstyle{definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{question}{Question}
\newtheorem*{problem}{Problem}
\newtheorem*{solution}{Solution}
\newtheorem{experiment}{Experiment}
\newtheorem{remark}[theorem]{Remark}

% % % % % % % % % % %our macros % % % % % % % % % % % % % % %
\newcommand{\esssup}{{\mathrm{ess}\sup}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\TT}{\mathbb{T}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\HH}{\mathcal{H}}
\newcommand{\Shah}{{\makebox[2.3ex][s]{$\sqcup$\hspace{-0.15em}\hfill $\sqcup$}  }}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\bc}{\textbf{c}}
\newcommand{\Wcp}{{W_0(L^p)}}
\newcommand{\lsp}{{\boldsymbol\ell}}
\newcommand{\Wco}{{W_0(L^1)}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\K}{\mathbb{K}}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\G}{\mathcal{G}}
\newcommand{\T}{\mathcal{T}}
\newcommand {\D} {\mathbb D}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\sspan}{\textnormal{span}}
\newcommand{\bracket}[1]{\left\langle#1\right\rangle}
\newcommand{\col}{\textnormal{Col}}
\newcommand{\row}{\textnormal{Row}}
\newcommand{\rank}{\textnormal{rank}}
\newcommand{\diag}{\textnormal{diag}}
\newcommand{\tr}{\textnormal{tr}}
\newcommand{\fro}{\textnormal{F}}
\newcommand{\rw}{\textnormal{rw}}
\newcommand{\sym}{\textnormal{sym}}
\newcommand{\eps}{\varepsilon}

\allowdisplaybreaks

\title{MATH 6310 (Fall 2025) Homework Solutions}
\author{Vandit Goel\\\textbf{UTA ID:}1002245699\\\textbf{Updated:} Wednesday  3 September, 2025}

\begin{document}

\maketitle

\section{Linear Algebra Problems}

\begin{problem}[1]
If $A\in\R^{n\times n}$ is symmetric, invertible, and has spectral decomposition $A=V\Lambda V^{-1}$, give (with proof) a formula for $A^{-1}$?

\begin{solution}
Let $A \in \mathbb{R}^{n \times n}$ be symmetric and invertible, with spectral decomposition
\[
A = V \Lambda V^{-1},
\]
where $V$ is an orthogonal matrix ($V^{-1} = V^\top$) and $\Lambda$ is diagonal.

Note: $\Lambda$ contains the eigenvalues of $A$, which are all non-zero since $A$ is invertible.
\begin{proof}
    \[Av=\Lambda v\]
    If some $\lambda_{i}=0$, then $Av_{i}=0$ for some vector $v_{i} \neq 0$. This makes the $\mathcal{N}(A)$ non-trivial, contradicting the assumption that $A$ is invertible.
\end{proof}

\vspace{\baselineskip}
\noindent To find $A^{-1}$, we use the property that for invertible matrices:
\[
(XYZ)^{-1} = Z^{-1} Y^{-1} X^{-1}
\]
\begin{align*}
	\Rightarrow(A)^{-1} &= (V \Lambda V^{-1})^{-1} \\
	&= (V^{-1})^{-1}\Lambda^{-1} V^{-1} \\
	&= V\Lambda^{-1} V^{-1}
\end{align*}
Thus, the inverse is given by
\[
A^{-1} = V \Lambda^{-1} V^{-1}.
\]
\end{solution}
\end{problem}


\begin{problem}[2]
\begin{enumerate}[(a)]
    \item Prove that $\mathcal{N}(A^\top A)=\mathcal{N}(A)$
    \item Prove that $\mathcal{N}(A)$ is orthogonal to $\row(A)$
    \item How does $\mathcal{N}(AB)$ relate to $\mathcal{N}(A)$ and $\mathcal{N}(B)$? Prove any statements you claim.
    \item Use Sylvester's Inequality to prove that if $A\in\R^{m\times r}$ and $B\in\R^{r\times n}$ have rank $r$, then $\rank(AB)=r$.
\end{enumerate}
\begin{solution}
    \begin{enumerate}[(a)]
        \item \begin{proof}
            Given $A\in\R^{m\times n}$, its \textbf{nullspace} is, 
            \[ \mathcal{N}(A):=\{x\in\R^n:Ax=0\} \]
            If $x\in\mathcal{N}(A)$, then
            \[A^\top(Ax)=A^\top0=0\]
            \[A^\top(Ax)=0\Leftrightarrow (A^\top A)x=0\]
            \[\Rightarrow x\in\mathcal{N}(A^\top A)\]
            This also shows that $\mathcal{N}(A) \subseteq \mathcal{N}(A^\top A)$

            \vspace{\baselineskip}
            \noindent If $x\in\mathcal{N}(A^\top A)$, then
            \[A^\top Ax=0\]
            Multiplying both sides by $x^\top$
            \[x^\top A^\top Ax=0\]
            \[\Rightarrow (Ax)^\top Ax=0\]
            \[\Rightarrow \norm{Ax}_2^2=0\]
            \[\Rightarrow Ax=0\] 
            \[\Rightarrow x\in\mathcal{N}(A)\] 
            This also shows that $\mathcal{N}(A^\top A) \subseteq \mathcal{N}(A) $
            
            \noindent Since both inclusions hold, we can conclude $\boldsymbol{\mathcal{N}(A^\top A)=\mathcal{N}(A)}$.
        \end{proof}
        \vspace{\baselineskip}
        \item \begin{claim}
        For any $x\in\mathcal{N}(A)$ and $y\in \row(A)$, $\mathcal{N}(A)$ is orthogonal to $\row(A)$ if
        \[\bracket{x,y}=0\]
        \end{claim}
        \begin{proof}
            We know $\row(A) = \col(A^\top)$. Also, $y$ can be written as a linear combination of the rows of A, 
            \[y=A^\top z : z\in\R^n\]
            Computing the dot product $\bracket{x,y} = 0$,. Hence, 
            \begin{align*}
            	\bracket{x,y} &= y^\top x \\
                &= (A^\top z)^\top x \\
                &= z^\top Ax \\
            \end{align*}
            We know $\mathcal{N}(A):=\{x\in\R^n:Ax=0\}$
            \begin{align*}
                &= z^\top 0 \\
                &= 0 
            \end{align*}
            Thus, we prove $\bracket{x,y} = 0$. 
            
            \noindent $\therefore$ $\mathcal{N}(A)$ is orthogonal to $\row(A)$ 
        \end{proof}
        \vspace{\baselineskip}
        \item \begin{claim}[1]
            \[ \mathcal{N}(B) \subseteq \mathcal{N}(AB)\]
        \end{claim}
        \begin{proof}
            We know $\mathcal{N}(B):=\{x\in\R^n:Bx=0\}$. If $Bx=0$, then
            \begin{align*}
                Bx &= 0 \\
                &\Rightarrow ABx = 0 \\
                &\Rightarrow (AB)x = 0 \\
                &\Rightarrow x \in \mathcal{N}(AB).
            \end{align*}
            Thus, this proves the nullspace of $B$ is a subspace of the nullspace of $AB$, i.e.
            \[\mathcal{N}(B) \subseteq \mathcal{N}(AB)\tag{1}\]
        \end{proof}
        \begin{claim}[2]
            $\mathcal{N}(AB)=\mathcal{N}(B) \leftrightarrow \operatorname{rank}(AB)=\operatorname{rank}(B)$ 
        \end{claim}
        \begin{proof}
            By the Rank-nullity theorem, Let $Z\in\R^{m\times n}$.  Then
            \[ \dim(\mathcal{N}(Z))+\rank(Z) = n \]
            Let $A\in \R^{m \times n}$, $B\in\R^{n \times p}$, $AB\in\R^{m\times p}$ and $\operatorname{rank}(AB)=\operatorname{rank}(B)$. Then
            \begin{align*}
                \dim(\mathcal{N}(AB)) + \operatorname{rank}(AB) &= \dim(\mathcal{N}(B)) + \operatorname{rank}(B) \\
                \Rightarrow\dim(\mathcal{N}(AB))&=\dim(\mathcal{N}(B))
                \tag{2}
            \end{align*}
            From (1) and (2) we can conclude,
            \[\mathcal{N}(AB)=\mathcal{N}(B)\]
        \end{proof} 
        \vspace{\baselineskip}
        \item \begin{proof}
            Given $A\in\R^{m\times r}$, $B\in\R^{r\times n}$ and $\operatorname{rank}(A) = \operatorname{rank}(B)=r$

            \noindent We know, $\rank(AB) \leq \min\{\rank(A), \rank(B)\}$. Then
            \[\rank(AB) \leq \min\{r, r\}\]
            \[\Rightarrow\rank(AB) \leq r\tag{1}\]
            By \textbf{Sylvester's Inequality:} If $A \in \R^{m\times r}, B \in \R^{r\times n}$, then 
            \begin{align*}
                &\rank(A) + \rank(B) \leq \rank(AB) + r \\
                &\Rightarrow r + r \leq \rank(AB) + r \\
                &\Rightarrow \rank(AB) \geq r \tag{2}
            \end{align*}

            By (1) and (2) we prove,
            \[\rank(AB) = r\]
        \end{proof}
    \end{enumerate}
\end{solution}
\end{problem}

\begin{problem}[13]
Let $A\in\R^{m\times n}$ have SVD $A=U\Sigma V^\top$.
\begin{enumerate}[(a)]
\item If $A=U\Sigma V^\top$ is invertible, give a formula for $A^{-1}$ in terms of $U$, $\Sigma$ and $V$. Justify your answer.
\item If $A=U\Sigma V^\top$, give a formula for $A^\top A$ and $AA^\top$. in terms of $U$, $\Sigma$, $V$. Justify your answer.
\end{enumerate}
\begin{solution}
    \begin{enumerate}[(a)]
        \item If $A=U\Sigma V^\top$ then
        \[A^{-1}=(U\Sigma V^\top)^{-1}=(V^\top)^{-1}\Sigma^{-1}U^{-1}\]
        For orthogoal matrices the inverse is the same as transpose. Because
        \[QQ^\top=I \]
        \[\Rightarrow A^{-1}=V\Sigma^{-1}U^{\top}\]
        \begin{proof}
            Given $A=U\Sigma V^\top$, multiplying both sides by $A^{-1}$
            \begin{align*}
                A^{-1}A &= (V\Sigma^{-1}U^{\top})(U\Sigma V^\top) \\
                A^{-1}A &= V\Sigma^{-1}\Sigma V^\top \because UU^\top=I \\
                A^{-1}A &= VV^\top \because \Sigma\Sigma^{-1}=I \\
                A^{-1}A &= I \because VV^\top=I
            \end{align*}
        \end{proof}
        \item Given $A=U\Sigma V^\top$, multiplying by $A^\top$ to the left
        \begin{align*}
            A^\top A&=(U\Sigma V^\top)^\top U\Sigma V^\top \\
            A^\top A&=(V^\top)^\top\Sigma^\top U^\top U\Sigma V^\top \\
            A^\top A&=V\Sigma^\top\Sigma V^\top \\
            A^\top A&=V\Sigma^2 V^\top
        \end{align*}
        Similarly multiplying $A^\top$ to the right
        \begin{align*}
            AA^\top &= U\Sigma V^\top (U\Sigma V^\top)^\top\\
            A^\top A&= U\Sigma V^\top (V^\top)^\top\Sigma^\top U^\top\\
            A^\top A&=U\Sigma^\top\Sigma U^\top \\
            A^\top A&=U\Sigma^2 U^\top
        \end{align*}
    \end{enumerate}
\end{solution}
\end{problem}

\begin{problem}[15]
Recall that the definition of the Frobenius norm is \[\|A\|_{\fro}:=\left(\sum_{i=1}^m\sum_{j=1}^n|A_{ij}|^2\right)^\frac12.\]
\begin{enumerate}[(a)]
\item Prove that the Frobenius norm is unitarily invariant (i.e., $\|QA\|_{\fro} = \|AW\|_{\fro} = \|A\|_{\fro}$ for any orthogonal matrices $Q,W$). \item Prove that for any $A\in\R^{m\times n}$,
 \[\|A\|_{\fro} = \left(\sum_{i=1}^{\rank(A)} \sigma_i^2\right)^\frac12.\]
\end{enumerate}
\begin{solution}
    \begin{enumerate}[(a)]
        \item \begin{claim}[1]
            $\|QAW\|_F=\|A\|_F$ $\forall$ orthogonal $Q,W$
        \end{claim}
        \begin{proof}
            For the Frobenius norm, using $\|A\|_F^{2} = \mathrm{trace}(A^\top A),$
            \[\|QAW\|_F^{2}=\mathrm{trace}(W^\top A^\top Q^\top.QAW)\] For $Q\in\R^{m\times n}$ it is orthogonal $\leftrightarrow Q^\top Q = I$
            \[\|QAW\|_F^{2}=\mathrm{trace}(W^\top A^\top AW)\]
            \[\|QAW\|_F^{2}=\mathrm{trace}( A^\top A) = \|A\|_F^{2}\]
        \end{proof} 
        \vspace{\baselineskip}
        \item \begin{proof}
            Let $SVD$ of $A$ be $A=U\Sigma V^\top$, where $U\in\R^{m\times m}$ and $V\in\R^{n\times n}$ are orthogonal and $\Sigma \in\R^{m\times n}$ \{diagonal\}
            \begin{align*}
            \|A\|_F^2&=\mathrm{trace}(A^\top A) \\
            &=\mathrm{trace}((U\Sigma V^\top)^\top U\Sigma V^\top) \\
            &= \mathrm{trace}(V\Sigma^\top U^\top U\Sigma V^\top) \\
            &= \mathrm{trace}(\Sigma^\top \Sigma)
            \end{align*}
            Let $r = \rank(A) \therefore$ the diagonal entries of $\Sigma = \{\sigma_1,\dots,\sigma_r\}$
            \begin{align*}
            \Rightarrow\|A\|_F^2&=\sum_{i=1}^{r}\sigma_{i}^{2} \\
            \Rightarrow\|A\|_F&=(\sum_{i=1}^{\rank(A)}\sigma_{i}^{2})^{\frac{1}{2}}
            \end{align*}
        \end{proof}
    \end{enumerate}
\end{solution}
\end{problem}
\begin{problem}[18][CUR Decomposition]\label{PROB:CUREasy}
Let $A\in\R^{m\times n}$, and $I\subset[m]$, $J\subset[n]$ with $|I|=|J|=k=\textnormal{rank}(A)$.  If $C=A(:,J)$, $R=A(I,:)$ and $U=A(I,J)$, prove that if $\rank(U)=k$, then $A=CU^{-1}R$.  (\textit{Hint:} First justify that one can write $A=CX$ for some unknown $X$. Next, write $R=P_IA$ where $P_I$ is a row selection matrix, and consider what happens to $A=CX$ upon multiplication by $P_I$.)

\textbf{Note:} If you google this, you will find my published proof of it. Please try to do this from scratch using the hint above... 
\begin{solution}
\begin{proof}
        Since matrix $C$ is subset of column vectors of $A$. Therefore, the $\text{Col}(C) \subseteq \text{Col}(A)$. We also know that $|I|=|J|=k=\textnormal{rank}(A)$.

    Assuming $\rank(U)=k$, the column vectors of $C$ become linearly independent i.e., $\operatorname{rank}(C)=k \Rightarrow \text{Col}(C)$ forms a basis for $\text{Col}(A)$. Thus, we can write $A=CX$ for some unknown $X$. Now, let $P_I$ be a row selection matrix that selects the rows indexed by $I$. Then we have:
    \[
    R = P_I A = P_I C X
    \]
    Since $P_I C$ selects the rows of $C$ corresponding to $I$, we can write:
    \[
    P_I C = U
    \Rightarrow R = U X
    \]
    \[
    A = C X = C U^{-1} R
    \]

\end{proof}
\end{solution}
\end{problem}

\end{document}

